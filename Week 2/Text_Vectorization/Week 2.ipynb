{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "503a20eb-3fe3-40dd-9078-234bb0e8a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0824b102-718c-4546-9ca6-5c8b27394c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'the sun is a star',\n",
    "    'the moon is a satellite',\n",
    "    'the sun and moon are celestial bodies'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5399ec4e-7ded-43fd-aaf9-0beada54fb11",
   "metadata": {},
   "source": [
    "Manual CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "375db2fb-1d7c-4cfa-8a59-8708847ea112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1.]]\n",
      "[[0 0 0 0 1 0 0 1 1 1]\n",
      " [0 0 0 0 1 1 1 0 0 1]\n",
      " [1 1 1 1 0 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "word_count = {}\n",
    "word_index = []\n",
    "for line in corpus:\n",
    "    for word in line.split(\" \"):\n",
    "        if word in word_count:\n",
    "            word_count[word] += 1\n",
    "        else:\n",
    "            word_index.append(word)\n",
    "            word_count[word] = 1\n",
    "final_transformed = np.zeros((len(corpus),len(word_count)))\n",
    "for i in range(len(corpus)):\n",
    "    for word in corpus[i].split(\" \"):\n",
    "        final_transformed[i][word_index.index(word)] += 1\n",
    "print(final_transformed)\n",
    "count_vect = CountVectorizer()\n",
    "count_vect.fit(corpus)\n",
    "X = count_vect.transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e90ead7-c903-4f8e-8055-1be4d2cf7ef9",
   "metadata": {},
   "source": [
    "Manual TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "85bd13c1-f0b5-4569-a460-265443ed43b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.34957775 0.45014501 0.45014501 0.34957775 0.59188659 0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.34957775 0.         0.45014501 0.34957775 0.         0.45014501\n",
      "  0.59188659 0.         0.         0.         0.        ]\n",
      " [0.25171084 0.32412354 0.         0.         0.         0.32412354\n",
      "  0.         0.4261835  0.4261835  0.4261835  0.4261835 ]]\n",
      "[[0.         0.         0.         0.         0.4804584  0.\n",
      "  0.         0.63174505 0.4804584  0.37311881]\n",
      " [0.         0.         0.         0.         0.4804584  0.4804584\n",
      "  0.63174505 0.         0.         0.37311881]\n",
      " [0.4261835  0.4261835  0.4261835  0.4261835  0.         0.32412354\n",
      "  0.         0.         0.32412354 0.25171084]]\n"
     ]
    }
   ],
   "source": [
    "len_corpus = len(corpus)\n",
    "word_index = []\n",
    "for line in corpus:\n",
    "    for word in line.split(\" \"):\n",
    "        if word not in word_index:\n",
    "            word_index.append(word)\n",
    "        index = word_index.index(word)\n",
    "len_word_index = len(word_index)\n",
    "document_appearances = np.zeros(len_word_index)\n",
    "for i in range(len_word_index):\n",
    "    word = word_index[i]\n",
    "    for j in range(len_corpus):\n",
    "        if word in corpus[j]:\n",
    "            document_appearances[i] += 1\n",
    "for i in range(len(document_appearances)):\n",
    "    document_appearances[i] = 1 + (math.log((len_corpus + 1) / (document_appearances[i] + 1)))\n",
    "final_transformed = np.zeros((len_corpus,len_word_index))\n",
    "for i in range(len_corpus):\n",
    "    temp_list = corpus[i].split(\" \")\n",
    "    len_temp_list = len(temp_list)\n",
    "    for word in temp_list:\n",
    "        final_transformed[i][word_index.index(word)] += 1\n",
    "    for word in temp_list:\n",
    "        index = word_index.index(word)\n",
    "        final_transformed[i][index] = final_transformed[i][index] * document_appearances[index]\n",
    "    final_transformed[i] = final_transformed[i] / np.sqrt(np.sum(final_transformed[i]**2))\n",
    "print(final_transformed)\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(corpus)\n",
    "X = tfidf_vect.transform(corpus)\n",
    "print(X.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
